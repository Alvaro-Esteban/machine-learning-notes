{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec010155",
   "metadata": {},
   "source": [
    "# üß† Week 03: Classification\n",
    "\n",
    "This week, you learn about **classification**, where your output variable $y$ can take on only one of a small handful of possible values, instead of any number in an infinite range. It turns out that **linear regression is not a good algorithm** for classification problems. Let's take a look at why ‚Äî this will lead us into a different algorithm called **logistic regression**.\n",
    "\n",
    "---\n",
    "\n",
    "## üü¢ Binary Classification\n",
    "\n",
    "A classic example is classifying a tumor as **malignant** versus **not**. In each of these problems, the variable you want to predict ($y$) can only take on **two possible values**: **No** or **Yes**, **0** or **1**.\n",
    "\n",
    "This type of classification problem is called **binary classification**, where the word *binary* refers to the fact that there are **only two classes or categories**.\n",
    "\n",
    "In these problems, the terms **class** and **category** are often used interchangeably.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Positive vs. Negative Class\n",
    "\n",
    "One common convention is to label:\n",
    "\n",
    "- The **false** or **0** class as the **negative class**  \n",
    "- The **true** or **1** class as the **positive class**\n",
    "\n",
    "For example:\n",
    "\n",
    "- In spam classification:\n",
    "  - A non-spam email ‚Üí **Negative example** ($y = 0$)\n",
    "  - A spam email ‚Üí **Positive example** ($y = 1$)\n",
    "\n",
    "This terminology helps clarify the nature of the outcome in binary classification.\n",
    "\n",
    "---\n",
    "\n",
    "## üî¢ Logistic Regression: Key Concepts\n",
    "\n",
    "### üßÆ Sigmoid (Logistic) Function\n",
    "\n",
    "The sigmoid function (also called the logistic function) maps real values to a range between 0 and 1:\n",
    "\n",
    "$$\n",
    "g(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "- If $z \\gg 0$, then $g(z) \\approx 1$\n",
    "- If $z \\ll 0$, then $g(z) \\approx 0$\n",
    "- If $z = 0$, then $g(z) = \\frac{1}{2}$\n",
    "\n",
    "---\n",
    "\n",
    "### üîÅ Logistic Regression Model\n",
    "\n",
    "Logistic regression uses the sigmoid function to map the linear combination of features to a probability between 0 and 1.\n",
    "\n",
    "**Step 1: Compute linear combination**\n",
    "\n",
    "$$\n",
    "z = \\mathbf{w} \\cdot \\mathbf{x} + b\n",
    "$$\n",
    "\n",
    "**Step 2: Apply sigmoid to compute prediction**\n",
    "\n",
    "$$\n",
    "f_{\\mathbf{w}, b}(\\mathbf{x}) = g(z) = \\frac{1}{1 + e^{-(\\mathbf{w} \\cdot \\mathbf{x} + b)}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Interpretation\n",
    "\n",
    "The output of the logistic regression model is interpreted as the **probability that $y=1$ given input $\\mathbf{x}$**:\n",
    "\n",
    "$$\n",
    "f_{\\mathbf{w}, b}(\\mathbf{x}) = P(y=1 \\mid \\mathbf{x}; \\mathbf{w}, b)\n",
    "$$\n",
    "\n",
    "Therefore:\n",
    "\n",
    "- If $f(\\mathbf{x}) = 0.7$, then the model predicts a 70% chance that $y=1$ and a 30% chance that $y=0$.\n",
    "- You can set a **decision threshold** (e.g., 0.5) to map probabilities to binary predictions.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Summary\n",
    "\n",
    "- Logistic regression models **binary classification** tasks ($y \\in \\{0, 1\\}$).\n",
    "- It applies a sigmoid function to a linear combination of inputs to output a probability.\n",
    "- The model output is interpreted as $P(y=1 \\mid \\mathbf{x})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6370ca",
   "metadata": {},
   "source": [
    "## üß† Logistic Regression with Multiple Features and Nonlinear Boundaries\n",
    "\n",
    "Let's now explore **logistic regression** with **two features** ($x_1$, $x_2$), and see how it can define both **linear** and **nonlinear** decision boundaries.\n",
    "\n",
    "---\n",
    "\n",
    "### üî¥üîµ Binary Classification with Two Features\n",
    "\n",
    "- Suppose you have a dataset with:\n",
    "  - **Red crosses**: $y = 1$ (positive class)\n",
    "  - **Blue circles**: $y = 0$ (negative class)\n",
    "\n",
    "- The logistic regression model makes predictions as:\n",
    "\n",
    "$$\n",
    "f(\\vec{x}) = g(z), \\quad \\text{where} \\quad z = w_1x_1 + w_2x_2 + b\n",
    "$$\n",
    "\n",
    "- Example:  \n",
    "  Let $w_1 = 1$, $w_2 = 1$, and $b = -3$.  \n",
    "  Then:\n",
    "  \n",
    "  $$\n",
    "  z = x_1 + x_2 - 3\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "### üü® Decision Boundary (Linear Case)\n",
    "\n",
    "- The **decision boundary** occurs when $z = 0$:\n",
    "  \n",
    "  $$\n",
    "  x_1 + x_2 - 3 = 0 \\quad \\Rightarrow \\quad x_1 + x_2 = 3\n",
    "  $$\n",
    "\n",
    "- This is a **straight line** separating the feature space:\n",
    "  - **Right of the line** $\\rightarrow$ predict $y = 1$\n",
    "  - **Left of the line** $\\rightarrow$ predict $y = 0$\n",
    "\n",
    "---\n",
    "\n",
    "### üîµ Non-Linear Decision Boundary (Using Polynomial Features)\n",
    "\n",
    "Now consider using **non-linear features**, such as:\n",
    "\n",
    "$$\n",
    "z = w_1 x_1^2 + w_2 x_2^2 + b\n",
    "$$\n",
    "\n",
    "- Let $w_1 = 1$, $w_2 = 1$, and $b = -1$  \n",
    "  Then:\n",
    "\n",
    "  $$\n",
    "  z = x_1^2 + x_2^2 - 1\n",
    "  $$\n",
    "\n",
    "- The decision boundary occurs when $z = 0$:\n",
    "\n",
    "  $$\n",
    "  x_1^2 + x_2^2 = 1\n",
    "  $$\n",
    "\n",
    "- This is a **circle** of radius 1 centered at (0,0):\n",
    "  - **Inside the circle** $\\rightarrow$ predict $y = 0$\n",
    "  - **Outside the circle** $\\rightarrow$ predict $y = 1$\n",
    "\n",
    "---\n",
    "\n",
    "### üî∑ Even More Complex Decision Boundaries\n",
    "\n",
    "We can introduce higher-order polynomial terms to allow more complex shapes:\n",
    "\n",
    "$$\n",
    "z = w_1 x_1 + w_2 x_2 + w_3 x_1^2 + w_4 x_1 x_2 + w_5 x_2^2\n",
    "$$\n",
    "\n",
    "- With appropriate parameters, logistic regression can define:\n",
    "  - **Ellipses**\n",
    "  - **Non-convex shapes**\n",
    "  - **Irregular complex regions**\n",
    "\n",
    "---\n",
    "\n",
    "### üìù Summary\n",
    "\n",
    "- Logistic regression with only linear terms results in **linear boundaries**.\n",
    "- By including **polynomial terms**, it can model **non-linear** and **complex regions**.\n",
    "- This flexibility makes logistic regression a powerful tool for binary classification tasks.\n",
    "\n",
    "In the next step, we‚Äôll learn how to **train** logistic regression using a cost function and gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62712f36",
   "metadata": {},
   "source": [
    "## üéØ Logistic Regression ‚Äì Cost Function and Loss\n",
    "\n",
    "### üß™ Goal\n",
    "Choose parameters $w$ and $b$ that **fit the training data well** for classification.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå Why Not Use Squared Error?\n",
    "\n",
    "- **Squared error** cost function works for linear regression:\n",
    "\n",
    "  $$\n",
    "  J(w, b) = \\frac{1}{m} \\sum_{i=1}^{m} \\frac{1}{2}(f^{(i)} - y^{(i)})^2\n",
    "  $$\n",
    "\n",
    "- But with **logistic regression**:\n",
    "  \n",
    "  $$\n",
    "  f(x) = \\frac{1}{1 + e^{-(w \\cdot x + b)}}\n",
    "  $$\n",
    "\n",
    "  - The resulting cost is **non-convex**.\n",
    "  - Leads to **many local minima**, making **gradient descent unreliable**.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Logistic Regression Loss Function\n",
    "\n",
    "We define a new **loss** function for a **single training example** $(x, y)$:\n",
    "\n",
    "$$\n",
    "L(f(x), y) =\n",
    "\\begin{cases}\n",
    "- \\log(f(x)) & \\text{if } y = 1 \\\\\\\\\n",
    "- \\log(1 - f(x)) & \\text{if } y = 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- Where $f(x)$ is the **predicted probability** from the sigmoid function.\n",
    "\n",
    "---\n",
    "\n",
    "### üìâ Intuition Behind the Loss\n",
    "\n",
    "- If $y = 1$:\n",
    "  - Predicting $f(x) \\approx 1$ ‚Üí ‚úÖ **Low loss**\n",
    "  - Predicting $f(x) \\approx 0$ ‚Üí ‚ùå **High loss ‚Üí $\\infty$**\n",
    "\n",
    "- If $y = 0$:\n",
    "  - Predicting $f(x) \\approx 0$ ‚Üí ‚úÖ **Low loss**\n",
    "  - Predicting $f(x) \\approx 1$ ‚Üí ‚ùå **High loss ‚Üí $\\infty$**\n",
    "\n",
    "üëâ The loss **heavily penalizes** wrong confident predictions.\n",
    "\n",
    "---\n",
    "\n",
    "### üßÆ Logistic Regression Cost Function (Average Loss)\n",
    "\n",
    "We define the **total cost function** $J(w, b)$ as the **average loss over $m$ training examples**:\n",
    "\n",
    "$$\n",
    "J(w, b) = \\frac{1}{m} \\sum_{i=1}^{m} L(f^{(i)}, y^{(i)})\n",
    "$$\n",
    "\n",
    "This cost is:\n",
    "\n",
    "- **Convex** ‚úÖ  \n",
    "- Works well with **gradient descent** ‚úÖ\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Summary\n",
    "\n",
    "- ‚ùå Squared error is **not appropriate** for logistic regression.\n",
    "- ‚úÖ New **log-loss** (or cross-entropy loss) is used:\n",
    "  - Convex\n",
    "  - Guides gradient descent properly\n",
    "- Produces **smooth cost surface** ‚Üí global minimum easier to find\n",
    "\n",
    "---\n",
    "\n",
    "In the next section, we‚Äôll simplify this cost function and begin **implementing gradient descent** to optimize parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee14e15",
   "metadata": {},
   "source": [
    "## üß† Logistic Regression ‚Äì Simplified Loss and Cost Functions\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Objective\n",
    "Simplify the **loss** and **cost** functions for easier implementation during **gradient descent**.\n",
    "\n",
    "---\n",
    "\n",
    "### üîÅ Generalized Loss Function (Binary Classification)\n",
    "\n",
    "Instead of writing two separate cases for $y = 1$ and $y = 0$, we can write the **loss** compactly as:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(f(x), y) = -y \\log(f(x)) - (1 - y)\\log(1 - f(x))\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Why This Works:\n",
    "\n",
    "- When $y = 1$:\n",
    "  - $1 - y = 0$\n",
    "  - $$\n",
    "    \\mathcal{L}(f(x), 1) = -\\log(f(x))\n",
    "    $$\n",
    "\n",
    "- When $y = 0$:\n",
    "  - $y = 0$\n",
    "  - $$\n",
    "    \\mathcal{L}(f(x), 0) = -\\log(1 - f(x))\n",
    "    $$\n",
    "\n",
    "Thus, this single equation works **for both cases**.\n",
    "\n",
    "---\n",
    "\n",
    "### üí∞ Cost Function (Average Loss Over Training Set)\n",
    "\n",
    "If you have $m$ training examples, the total **cost function** becomes:\n",
    "\n",
    "$$\n",
    "J(w, b) = \\frac{1}{m} \\sum_{i=1}^{m} \\mathcal{L}(f^{(i)}, y^{(i)})\n",
    "$$\n",
    "\n",
    "Substitute the expression for $\\mathcal{L}$:\n",
    "\n",
    "$$\n",
    "J(w, b) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(f^{(i)}) + (1 - y^{(i)})\\log(1 - f(x^{(i)})) \\right]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $f^{(i)} = \\frac{1}{1 + e^{-(w \\cdot x^{(i)} + b)}}$ is the **sigmoid prediction**.\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Notes:\n",
    "\n",
    "- This cost function is **convex**, which makes **gradient descent reliable**.\n",
    "- It's derived from **Maximum Likelihood Estimation (MLE)** ‚Äî a common statistical principle for fitting models.\n",
    "- This is the **standard loss used in binary logistic regression**.\n",
    "\n",
    "---\n",
    "\n",
    "### üîß Implementation Hint\n",
    "\n",
    "In practice, this function is:\n",
    "\n",
    "- Simple to code\n",
    "- Easy to differentiate (for gradient descent)\n",
    "- Robust for classification tasks\n",
    "\n",
    "‚û°Ô∏è Let‚Äôs now move on to **implementing gradient descent** for this cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a70a924",
   "metadata": {},
   "source": [
    "## üßÆ Logistic Regression - Gradient Descent Implementation\n",
    "\n",
    "To fit the parameters of a **logistic regression model**, we aim to **minimize the cost function**:\n",
    "\n",
    "$$\n",
    "J(\\mathbf{w}, b) = \\frac{1}{m} \\sum_{i=1}^{m} \\mathcal{L}(f_{\\mathbf{w}, b}(\\mathbf{x}^{(i)}), y^{(i)})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{w}$ are the weights\n",
    "- $b$ is the bias\n",
    "- $f(\\mathbf{x})$ is the model output (sigmoid of $z = \\mathbf{w} \\cdot \\mathbf{x} + b$)\n",
    "- $\\mathcal{L}$ is the **logistic loss**:\n",
    "  $$\n",
    "  \\mathcal{L}(f, y) = -y \\log(f) - (1 - y) \\log(1 - f)\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "### üîÅ Gradient Descent Algorithm\n",
    "\n",
    "To minimize the cost $J(\\mathbf{w}, b)$, apply **gradient descent**:\n",
    "\n",
    "#### üß† Derivatives:\n",
    "\n",
    "- For each weight $w_j$:\n",
    "  $$\n",
    "  \\frac{\\partial J(\\mathbf{w}, b)}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^{m} \\left(f_{\\mathbf{w}, b}(\\mathbf{x}^{(i)}) - y^{(i)}\\right) \\cdot x_j^{(i)}\n",
    "  $$\n",
    "\n",
    "- For the bias $b$:\n",
    "  $$\n",
    "  \\frac{\\partial J(\\mathbf{w}, b)}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^{m} \\left(f_{\\mathbf{w}, b}(\\mathbf{x}^{(i)}) - y^{(i)}\\right)\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "### üîÑ Parameter Updates\n",
    "\n",
    "Simultaneous updates:\n",
    "\n",
    "$$\n",
    "w_j := w_j - \\alpha \\cdot \\frac{\\partial J}{\\partial w_j} \\quad \\text{(for all } j=1,\\dots,n\\text{)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b := b - \\alpha \\cdot \\frac{\\partial J}{\\partial b}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\alpha$ is the **learning rate**\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùó Important Notes\n",
    "\n",
    "- The equations **look similar** to those in linear regression, but:\n",
    "  - In **linear regression**, $f(\\mathbf{x}) = \\mathbf{w} \\cdot \\mathbf{x} + b$\n",
    "  - In **logistic regression**, $f(\\mathbf{x}) = \\sigma(\\mathbf{w} \\cdot \\mathbf{x} + b)$ where $\\sigma(z)$ is the **sigmoid function**\n",
    "\n",
    "- So the **behavior is different**, despite the gradient formulas being similar in form.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö° Optimization Tip: Feature Scaling\n",
    "\n",
    "Feature scaling (e.g., normalizing inputs to range $[-1, 1]$) helps **speed up convergence** of gradient descent ‚Äî just like in linear regression.\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ Labs\n",
    "\n",
    "You‚Äôll implement this in the optional/practice labs:\n",
    "- Visualize: sigmoid function, contour plots, 3D surface, learning curves\n",
    "- Use **scikit-learn** for logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6b3280",
   "metadata": {},
   "source": [
    "## üß† The Problem of Overfitting\n",
    "\n",
    "### ‚úÖ Concepts Introduced\n",
    "\n",
    "- **Overfitting**: When a model fits the training data *too well*, including the noise ‚Äî resulting in poor generalization to new/unseen data.\n",
    "- **Underfitting**: When a model is *too simple* to capture the underlying trend in the data.\n",
    "- **Generalization**: The ability of a model to perform well on **new data**, not just the training data.\n",
    "\n",
    "---\n",
    "\n",
    "### üìâ Example: Housing Prices (Regression)\n",
    "\n",
    "#### Underfitting (High Bias)\n",
    "- Model: Simple linear regression.\n",
    "- Fits the data poorly.\n",
    "- Misses obvious patterns.\n",
    "- Cost is high.\n",
    "- **Too simple.**\n",
    "\n",
    "#### Just Right (Balanced Bias-Variance)\n",
    "- Model: Quadratic regression (`x`, `x¬≤`).\n",
    "- Fits the data reasonably well.\n",
    "- Generalizes well to new houses.\n",
    "- **Best balance**.\n",
    "\n",
    "#### Overfitting (High Variance)\n",
    "- Model: 4th-degree polynomial (`x`, `x¬≤`, `x¬≥`, `x‚Å¥`).\n",
    "- Fits the training data *perfectly* (cost = 0).\n",
    "- Very **wiggly**, poor predictions for new data.\n",
    "- Sensitive to slight changes in training set.\n",
    "- **Too complex.**\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Classification Example (Logistic Regression)\n",
    "\n",
    "- Inputs: `x‚ÇÅ = tumor size`, `x‚ÇÇ = patient age`\n",
    "- Labels: Malignant (√ó) vs Benign (‚óã)\n",
    "\n",
    "| Model Type        | Decision Boundary          | Behavior      |\n",
    "|-------------------|-----------------------------|---------------|\n",
    "| **Underfit**      | Linear (`z = w·µÄx + b`)       | High bias     |\n",
    "| **Just Right**    | Quadratic (`x‚ÇÅ¬≤`, `x‚ÇÇ¬≤`, etc.) | Good fit      |\n",
    "| **Overfit**       | High-order polynomial        | High variance |\n",
    "\n",
    "---\n",
    "\n",
    "### üß© Terminology\n",
    "\n",
    "- **High Bias**: Model too simplistic; underfits data.\n",
    "- **High Variance**: Model too complex; overfits data.\n",
    "- **Just Right**: Good generalization, low cost on new data.\n",
    "\n",
    "> üí¨ Like the story of *Goldilocks*:  \n",
    "> ‚ùÑ Too cold ‚Üí Underfit (High Bias)  \n",
    "> üî• Too hot ‚Üí Overfit (High Variance)  \n",
    "> üç≤ Just right ‚Üí Balanced model\n",
    "\n",
    "---\n",
    "\n",
    "### üõ†Ô∏è What‚Äôs next?\n",
    "\n",
    "In the next video:  \n",
    "‚û°Ô∏è **Regularization**: A technique to reduce overfitting and help the model generalize better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24236ea",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Addressing Overfitting\n",
    "\n",
    "Overfitting occurs when a model fits the training data *too well*, capturing noise and leading to poor generalization. In this video, three main strategies to reduce overfitting are introduced:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. üìà Collect More Training Data\n",
    "- More data helps the model generalize better.\n",
    "- Reduces the model‚Äôs tendency to memorize noise.\n",
    "- Particularly useful for complex models with many features.\n",
    "- **Limitation**: Not always feasible (e.g., limited housing sales in an area).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. üß© Use Fewer Features (Feature Selection)\n",
    "- Using too many features (especially irrelevant ones) can cause overfitting.\n",
    "- Reducing the feature set helps simplify the model.\n",
    "- This process is called **feature selection**.\n",
    "\n",
    "#### Example:\n",
    "Instead of using 100 features (size, bedrooms, floors, age, income, distance to coffee shop, etc.), choose a subset like:\n",
    "- Size\n",
    "- Number of bedrooms\n",
    "- Age of the house\n",
    "\n",
    "> ‚ö†Ô∏è Caveat: You might discard useful information by eliminating features.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. üß≤ Apply Regularization\n",
    "Regularization **shrinks parameter values** to reduce overfitting without eliminating features.\n",
    "\n",
    "#### üîé Key Ideas:\n",
    "- Overfit models often have **large weights** (`w‚ÇÅ, w‚ÇÇ, ..., w‚Çô`).\n",
    "- Regularization encourages smaller weights.\n",
    "- Keeps **all features**, but limits their impact.\n",
    "\n",
    "#### ‚úÖ Advantage:\n",
    "- Smooths the model without dropping features completely.\n",
    "- Especially useful in models like polynomial regression or logistic regression.\n",
    "\n",
    "#### üìå Convention:\n",
    "- Regularize only the weights (`w‚ÇÅ...w‚Çô`), not the bias term `b`.\n",
    "- Regularizing `b` makes minimal difference in practice.\n",
    "\n",
    "---\n",
    "\n",
    "### üìö Summary: 3 Ways to Reduce Overfitting\n",
    "\n",
    "| Strategy                | Description                                                     |\n",
    "|-------------------------|-----------------------------------------------------------------|\n",
    "| 1. More Data            | Improves generalization, reduces model variance.               |\n",
    "| 2. Fewer Features       | Simplifies model, avoids irrelevant/noisy data.                |\n",
    "| 3. Regularization       | Penalizes large weights, reduces model complexity smoothly.    |\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ Optional Lab\n",
    "In the optional lab:\n",
    "- Play with synthetic regression/classification data.\n",
    "- Add/remove data points.\n",
    "- Adjust polynomial degree (`x`, `x¬≤`, `x¬≥`, etc.).\n",
    "- Try adding/removing features.\n",
    "- **Visualize** the impact of overfitting and how each strategy mitigates it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a5dc0b",
   "metadata": {},
   "source": [
    "## üßÆ Cost Function with Regularization\n",
    "\n",
    "### üéØ Goal\n",
    "Incorporate **regularization** into the model to prevent **overfitting** by penalizing large values of the parameters $w_j$.\n",
    "\n",
    "---\n",
    "\n",
    "### üîÅ Cost Function (Linear Regression)\n",
    "\n",
    "The original (non-regularized) cost function is:\n",
    "\n",
    "$$\n",
    "J(w, b) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( f^{(i)} - y^{(i)} \\right)^2\n",
    "$$\n",
    "\n",
    "> where $f^{(i)} = w^T x^{(i)} + b$\n",
    "\n",
    "---\n",
    "\n",
    "### üõ°Ô∏è Regularization Term\n",
    "\n",
    "We add a penalty on the size of the weights:\n",
    "\n",
    "$$\n",
    "\\text{Regularization term} = \\frac{\\lambda}{2m} \\sum_{j=1}^{n} w_j^2\n",
    "$$\n",
    "\n",
    "- Penalizes large values of $w_j$\n",
    "- The bias term $b$ is **not** typically regularized\n",
    "- $\\lambda$ is the **regularization parameter** that controls the strength of the penalty\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Regularized Cost Function\n",
    "\n",
    "The full cost function with regularization becomes:\n",
    "\n",
    "$$\n",
    "J_{\\text{reg}}(w, b) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( f^{(i)} - y^{(i)} \\right)^2 + \\frac{\\lambda}{2m} \\sum_{j=1}^{n} w_j^2\n",
    "$$\n",
    "\n",
    "- The first term minimizes the prediction error\n",
    "- The second term keeps the model weights small to reduce overfitting\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öñÔ∏è Effect of $\\lambda$\n",
    "\n",
    "| Value of $\\lambda$         | Model Behavior                            |\n",
    "|----------------------------|-------------------------------------------|\n",
    "| $\\lambda = 0$              | No regularization ‚Üí ‚ùóÔ∏è Overfitting         |\n",
    "| $\\lambda \\to \\infty$       | All weights $\\to 0$ ‚Üí ‚ùóÔ∏è Underfitting      |\n",
    "| Intermediate $\\lambda$     | üü¢ Balanced fit with reduced complexity    |\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Notes\n",
    "\n",
    "- Regularization is especially useful when you have **many features** and are unsure which ones are important\n",
    "- It allows you to **keep all features** while reducing their influence\n",
    "- By convention, **only** $w_1, ..., w_n$ are regularized (not $b$)\n",
    "- The $1 / (2m)$ scaling helps make $\\lambda$ more consistent across different dataset sizes\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Intuition\n",
    "\n",
    "Regularization ‚âà Enforcing simpler models:\n",
    "\n",
    "- Less ‚Äúwiggly‚Äù / less complex functions\n",
    "- Better generalization to new data\n",
    "- Prevents the model from fitting to noise\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to write the next part on **Regularization in Linear Regression** ‚Äî with gradients, partial derivatives, and the updated gradient descent formulas?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fe3572",
   "metadata": {},
   "source": [
    "## üßÆ Regularized Linear Regression\n",
    "\n",
    "In regularized linear regression, we apply **gradient descent** to minimize the following cost function:\n",
    "\n",
    "$$\n",
    "J(w, b) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( f^{(i)} - y^{(i)} \\right)^2 + \\frac{\\lambda}{2m} \\sum_{j=1}^{n} w_j^2\n",
    "$$\n",
    "\n",
    "> where $f^{(i)} = w^T x^{(i)} + b$\n",
    "\n",
    "---\n",
    "\n",
    "### üîÅ Gradient Descent Updates\n",
    "\n",
    "For each parameter $w_j$ (with $j = 1, \\dots, n$):\n",
    "\n",
    "$$\n",
    "w_j := w_j - \\alpha \\left( \\frac{1}{m} \\sum_{i=1}^{m} \\left( f(x^{(i)}) - y^{(i)} \\right) x_j^{(i)} + \\frac{\\lambda}{m} w_j \\right)\n",
    "$$\n",
    "\n",
    "For the bias $b$ (not regularized):\n",
    "\n",
    "$$\n",
    "b := b - \\alpha \\left( \\frac{1}{m} \\sum_{i=1}^{m} \\left( f(x^{(i)}) - y^{(i)} \\right) \\right)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Summary\n",
    "\n",
    "- Regularization adds a term to the gradient update that **shrinks** weights\n",
    "- Helps prevent **overfitting** by discouraging large parameter values\n",
    "- Bias term $b$ is **not** regularized by default\n",
    "- Works especially well when you have **many features** and **not much data**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bfc073",
   "metadata": {},
   "source": [
    "## üß™ Regularized Logistic Regression\n",
    "\n",
    "Logistic regression can **overfit** when trained with many features ‚Äî especially **high-order polynomials**. Regularization helps reduce overfitting by penalizing large parameter values.\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Regularized Cost Function\n",
    "\n",
    "To regularize logistic regression, we modify the original cost function:\n",
    "\n",
    "$$\n",
    "J(w, b) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(f(x^{(i)})) + (1 - y^{(i)}) \\log(1 - f(x^{(i)})) \\right] + \\frac{\\lambda}{2m} \\sum_{j=1}^{n} w_j^2\n",
    "$$\n",
    "\n",
    "> - $f(x^{(i)}) = \\sigma(z^{(i)}) = \\frac{1}{1 + e^{-z^{(i)}}}$  \n",
    "> - $z^{(i)} = w^T x^{(i)} + b$\n",
    "\n",
    "- The second term is the **regularization term**, which penalizes large values of $w_j$\n",
    "- We do **not** regularize the bias term $b$\n",
    "\n",
    "---\n",
    "\n",
    "### üîÅ Gradient Descent Updates\n",
    "\n",
    "We minimize $J(w, b)$ using gradient descent with the following **update rules**:\n",
    "\n",
    "#### For each weight $w_j$:\n",
    "\n",
    "$$\n",
    "w_j := w_j - \\alpha \\left( \\frac{1}{m} \\sum_{i=1}^{m} (f^{(i)} - y^{(i)}) x_j^{(i)} + \\frac{\\lambda}{m} w_j \\right)\n",
    "$$\n",
    "\n",
    "#### For the bias $b$ (not regularized):\n",
    "\n",
    "$$\n",
    "b := b - \\alpha \\left( \\frac{1}{m} \\sum_{i=1}^{m} (f^{(i)} - y^{(i)}) \\right)\n",
    "$$\n",
    "\n",
    "- $\\alpha$ is the learning rate  \n",
    "- $\\lambda$ is the regularization parameter  \n",
    "- $m$ is the number of training examples\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Key Insights\n",
    "\n",
    "- The update for $w_j$ is **identical** in form to regularized linear regression\n",
    "- Only difference: $f^{(i)}$ is now the **sigmoid** of $z^{(i)}$, not a linear function\n",
    "- Regularization **shrinks** the weights $w_j$, helping to prevent overfitting\n",
    "- $b$ is **excluded** from regularization to avoid unnecessary bias shifts\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Summary\n",
    "\n",
    "- Regularization improves generalization in logistic regression by **penalizing large weights**\n",
    "- The effect is to smooth the decision boundary and **avoid overfitting**, even with many features\n",
    "- Update rules are nearly identical to those for regularized linear regression\n",
    "\n",
    "---\n",
    "\n",
    "üî¨ You‚Äôll implement this in the practice lab. Try adjusting $\\lambda$ interactively and observe how it affects the decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2ebf70",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
