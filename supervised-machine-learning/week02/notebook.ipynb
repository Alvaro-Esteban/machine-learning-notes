{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c0e1159",
   "metadata": {},
   "source": [
    "## üè† Week 02 ‚Äì Regression with Multiple Input Variables\n",
    "\n",
    "In the original version of linear regression, you had a single feature \\( x \\), the size of the house, and you were able to predict \\( y \\), the price of the house.  \n",
    "The model was:\n",
    "\n",
    "$$\n",
    "f_{w,b}(x) = wx + b \\tag{1}\n",
    "$$\n",
    "\n",
    "But now, what if you did not only have the size of the house as a feature to predict the price, but also the **number of bedrooms**, **number of floors**, and **age of the home in years**?\n",
    "\n",
    "That gives you much more information to predict the price.\n",
    "\n",
    "---\n",
    "\n",
    "### üßÆ Notation\n",
    "\n",
    "Let:\n",
    "\n",
    "- $\\vec{x} = [x_1, x_2, x_3, x_4]$: the four input features (e.g. size, bedrooms, floors, age)  \n",
    "- $x_j$: the $j$-th feature, with $j = 1, 2, ..., n$  \n",
    "- $\\vec{x}^{(i)} = [x_1^{(i)}, x_2^{(i)}, ..., x_n^{(i)}]$: feature vector of the $i$-th training example  \n",
    "- $n$: number of features (e.g., $n = 4$)\n",
    "\n",
    "So for a training example \\( i \\), we have:\n",
    "\n",
    "$$\n",
    "f_{w,b}(x^{(i)}) = w_1 x_1^{(i)} + w_2 x_2^{(i)} + w_3 x_3^{(i)} + w_4 x_4^{(i)} + b \\tag{2}\n",
    "$$\n",
    "\n",
    "Or more generally, for \\( n \\) features:\n",
    "\n",
    "$$\n",
    "f_{w,b}(x^{(i)}) = \\sum_{j=1}^{n} w_j x_j^{(i)} + b \\tag{3}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### üìê Vectors Notation\n",
    "\n",
    "Let:\n",
    "\n",
    "- $\\vec{w} = [w_1, w_2, ..., w_n]$: vector of weights  \n",
    "- $\\vec{x}^{(i)} = [x_1^{(i)}, x_2^{(i)}, ..., x_n^{(i)}]$: feature vector  \n",
    "- $b$: bias (a scalar)\n",
    "\n",
    "Then the model becomes:\n",
    "\n",
    "$$\n",
    "f_{w,b}(x^{(i)}) = \\vec{w} \\cdot \\vec{x}^{(i)} + b \\tag{4}\n",
    "$$\n",
    "\n",
    "This uses the **dot product** of two vectors:\n",
    "\n",
    "$$\n",
    "\\vec{w} \\cdot \\vec{x}^{(i)} = \\sum_{j=1}^{n} w_j x_j^{(i)} \\tag{5}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Name of the Model\n",
    "\n",
    "This model is called **multiple linear regression** (not *multivariate regression* ‚Äî that refers to predicting multiple outputs, which is a different topic).\n",
    "\n",
    "It's the natural extension of **univariate linear regression** (with one feature) to multiple input features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b74954",
   "metadata": {},
   "source": [
    "## üßÆ Vectorization\n",
    "\n",
    "Vectorization is a technique to implement learning algorithms **more efficiently** ‚Äî both in terms of **execution time** and **code clarity**. It allows you to take advantage of optimized **linear algebra libraries** (e.g., NumPy) and even **GPU acceleration**.\n",
    "\n",
    "---\n",
    "\n",
    "### Parameters and Features\n",
    "\n",
    "Let:\n",
    "\n",
    "- $\\vec{w} = [w_1, w_2, w_3]$ with $n = 3$: weight vector  \n",
    "- $\\vec{x} = [x_1, x_2, x_3]$: feature vector  \n",
    "- $b$: bias (a scalar)\n",
    "\n",
    "In NumPy (Python):\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "w = np.array([1.0, 2.5, -3.3])\n",
    "x = np.array([10, 20, 30])\n",
    "b = 4\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå Without Vectorization\n",
    "\n",
    "**Manual computation (bad for large $n$):**\n",
    "\n",
    "$$\n",
    "f_{\\vec{w}, b}(\\vec{x}) = w_1 x_1 + w_2 x_2 + w_3 x_3 + b\n",
    "$$\n",
    "\n",
    "**Using a for loop:**\n",
    "\n",
    "```python\n",
    "f = 0\n",
    "for j in range(0, n):\n",
    "    f += w[j] * x[j]\n",
    "f += b\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ With Vectorization\n",
    "\n",
    "**Mathematical expression:**\n",
    "\n",
    "$$\n",
    "f_{\\vec{w}, b}(\\vec{x}) = \\vec{w} \\cdot \\vec{x} + b\n",
    "$$\n",
    "\n",
    "**Python (NumPy):**\n",
    "\n",
    "```python\n",
    "f = np.dot(w, x) + b\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ Benefits of Vectorization\n",
    "\n",
    "- ‚úÖ **Shorter, cleaner code**\n",
    "- ‚úÖ **Faster execution**, especially for large $n$\n",
    "- ‚úÖ Utilizes optimized backend libraries (e.g., BLAS, LAPACK, GPUs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a8f175",
   "metadata": {},
   "source": [
    "## Gradient Descent for Multiple Linear Regression\n",
    "\n",
    "We're going to repeatedly update each parameter $w_j$ using the rule:\n",
    "\n",
    "$$\n",
    "w_j := w_j - \\alpha \\frac{\\partial J(\\vec{w}, b)}{\\partial w_j}\n",
    "$$\n",
    "\n",
    "And the bias $b$ using:\n",
    "\n",
    "$$\n",
    "b := b - \\alpha \\frac{\\partial J(\\vec{w}, b)}{\\partial b}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\alpha$ is the learning rate  \n",
    "- $J(\\vec{w}, b)$ is the cost function  \n",
    "- $w_j$ refers to the $j$-th weight in the parameter vector $\\vec{w}$  \n",
    "\n",
    "---\n",
    "\n",
    "### üß† Intuition\n",
    "\n",
    "In univariate regression (one feature), we had the update rules:\n",
    "\n",
    "$$\n",
    "w := w - \\alpha \\frac{\\partial J(w, b)}{\\partial w}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b := b - \\alpha \\frac{\\partial J(w, b)}{\\partial b}\n",
    "$$\n",
    "\n",
    "Now, with **multiple features** ($n \\geq 2$), we update **each weight** $w_j$ individually:\n",
    "\n",
    "$$\n",
    "w_j := w_j - \\alpha \\left( \\frac{1}{m} \\sum_{i=1}^{m} \\left( f_{\\vec{w}, b}(\\vec{x}^{(i)}) - y^{(i)} \\right) \\cdot x_j^{(i)} \\right)\n",
    "$$\n",
    "\n",
    "And we update $b$ as:\n",
    "\n",
    "$$\n",
    "b := b - \\alpha \\left( \\frac{1}{m} \\sum_{i=1}^{m} \\left( f_{\\vec{w}, b}(\\vec{x}^{(i)}) - y^{(i)} \\right) \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $f_{\\vec{w}, b}(\\vec{x}^{(i)}) = \\vec{w} \\cdot \\vec{x}^{(i)} + b$ is the prediction\n",
    "- $x_j^{(i)}$ is the $j$-th feature of the $i$-th example\n",
    "- $m$ is the number of training examples\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ Implementation note\n",
    "\n",
    "Instead of hardcoding 3 or 4 parameters, you‚Äôll now use **loops over $j = 1..n$** to update all weights $\\vec{w}$. The update for $b$ stays similar.\n",
    "\n",
    "---\n",
    "\n",
    "### üßÆ Bonus: The Normal Equation\n",
    "\n",
    "There's another method that can compute the **optimal weights $\\vec{w}$ and bias $b$ without iterations**, called the **normal equation**. It's based on linear algebra.\n",
    "\n",
    "‚úÖ Pros:\n",
    "- No need to choose learning rate $\\alpha$\n",
    "- No need for iteration\n",
    "\n",
    "‚ö†Ô∏è Cons:\n",
    "- Computationally expensive for large $n$\n",
    "- Not generalizable to other algorithms (like logistic regression or neural networks)\n",
    "\n",
    "That's why in practice, most implementations still use **gradient descent** or **advanced solvers**, even for linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b362d7",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è Feature Scaling\n",
    "\n",
    "Feature scaling is a technique used to **normalize the range of independent variables or features** in your data. It ensures that all features contribute equally to the learning algorithm, especially for **gradient descent**, where features with larger scales can dominate the cost function and slow down convergence.\n",
    "\n",
    "### üìå Why is feature scaling important?\n",
    "\n",
    "- Features may have **different units** (e.g., size in m¬≤, age in years, price in $).\n",
    "- Without scaling, **gradient descent may converge slowly** or get stuck.\n",
    "- It helps to **improve numerical stability**.\n",
    "- Needed for algorithms that compute distances (e.g., **k-NN**, **SVM**, **logistic regression**).\n",
    "\n",
    "### ‚úÖ When to scale\n",
    "\n",
    "Feature scaling is **recommended** when:\n",
    "- You're using **gradient-based algorithms** (like linear regression, logistic regression, neural networks).\n",
    "- You have features with **different magnitudes** or units.\n",
    "- Your model does not handle feature scaling internally.\n",
    "\n",
    "---\n",
    "\n",
    "### üî¢ Common Methods of Feature Scaling\n",
    "\n",
    "Feature scaling ensures that all input features contribute equally to the model by adjusting their ranges or distributions. Here are the most common methods:\n",
    "\n",
    "- **Min-Max Scaling:** Subtract the minimum and divide by the range (max - min). Rescales features to lie between 0 and 1.\n",
    "- **Mean Normalization:** Subtract the mean and divide by the range. Centers the data around 0.\n",
    "- **Z-score Normalization (Standardization):** Subtract the mean and divide by the standard deviation. Ensures zero mean and unit variance.\n",
    "\n",
    "| Method                 | Formula                                                  | Typical Range        |\n",
    "|------------------------|----------------------------------------------------------|----------------------|\n",
    "| **Min-Max Scaling**    | $x' = \\dfrac{x - \\min(x)}{\\max(x) - \\min(x)}$            | $[0,\\ 1]$            |\n",
    "| **Mean Normalization** | $x' = \\dfrac{x - \\mu}{\\max(x) - \\min(x)}$                | Around 0             |\n",
    "| **Standardization**    | $x' = \\dfrac{x - \\mu}{\\sigma}$                           | Mean = 0, Std = 1    |\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Example in Python (Standardization)\n",
    "\n",
    "```python\n",
    "# Assuming X is a NumPy array of shape (m, n)\n",
    "mu     = np.mean(X, axis=0)   # mean for each feature\n",
    "sigma  = np.std(X, axis=0)    # standard deviation\n",
    "X_norm = (X - mu) / sigma     # standardized features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7420c769",
   "metadata": {},
   "source": [
    "## üìâ How to Tell if Gradient Descent is Converging\n",
    "\n",
    "To verify if gradient descent is working properly (i.e., minimizing the cost function \\( J(w, b) \\)), it helps to analyze the **learning curve** ‚Äî a plot of the cost function over iterations.\n",
    "\n",
    "### üîç Key Concepts\n",
    "\n",
    "- **Gradient Descent Update Rule:**\n",
    "  $$ w := w - \\alpha \\frac{\\partial J(w, b)}{\\partial w}, \\quad b := b - \\alpha \\frac{\\partial J(w, b)}{\\partial b} $$\n",
    "\n",
    "- **Learning Curve:**\n",
    "  - Horizontal axis: Number of iterations\n",
    "  - Vertical axis: Cost function \\( J(w, b) \\)\n",
    "  - Each point: Value of the cost after one update of \\( w \\) and \\( b \\)\n",
    "  - Purpose: Visualize if the cost is decreasing steadily\n",
    "\n",
    "### ‚úÖ Signs Gradient Descent is Working\n",
    "\n",
    "- The cost \\( J(w, b) \\) **decreases after every iteration**\n",
    "- The curve **smoothly flattens out**, indicating convergence\n",
    "- Cost function becomes stable (i.e., little to no change)\n",
    "\n",
    "### ‚ùå Signs Something is Wrong\n",
    "\n",
    "- Cost **increases** after an iteration ‚Üí likely:\n",
    "  - Learning rate \\( \\alpha \\) is too large\n",
    "  - Bug in the implementation\n",
    "- Curve is **not flattening** ‚Üí may not be converging\n",
    "\n",
    "### üìà Convergence Criteria\n",
    "\n",
    "- **Visual inspection** is the most reliable:\n",
    "  - Helps detect problems early (e.g., bad learning rate)\n",
    "  - Shows whether training should stop\n",
    "- **Automatic test (epsilon rule):**\n",
    "  - Define a small threshold \\( \\varepsilon \\), e.g., 0.001\n",
    "  - If cost decrease \\( < \\varepsilon \\), declare convergence\n",
    "  - Less preferred due to difficulty in choosing a good \\( \\varepsilon \\)\n",
    "\n",
    "### ‚è≥ Number of Iterations\n",
    "\n",
    "- Varies **greatly** depending on the application:\n",
    "  - Some models converge in 30 iterations\n",
    "  - Others may take 1,000 or 100,000\n",
    "- Hence, plotting the learning curve is more reliable than predefining a number\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89edcac",
   "metadata": {},
   "source": [
    "## üîß Choosing a Good Learning Rate (Œ±)\n",
    "\n",
    "Selecting the right learning rate is **crucial** for gradient descent to work effectively.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† What Can Go Wrong?\n",
    "\n",
    "#### üî∏ Learning rate too **large**:\n",
    "- Gradient descent **does not converge**\n",
    "- Cost function `J(w,b)` may **oscillate** or even **increase**\n",
    "- Can **overshoot** the minimum repeatedly\n",
    "- Possible graph shape: zig-zag or upward curve\n",
    "\n",
    "#### üî∏ Learning rate too **small**:\n",
    "- Gradient descent **converges very slowly**\n",
    "- Training takes many iterations\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Good Gradient Descent Behavior\n",
    "\n",
    "- Cost \\( J(w,b) \\) **decreases on every iteration**\n",
    "- Learning curve is smooth and decreasing\n",
    "- Converges towards a stable minimum\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ Debugging Tip\n",
    "\n",
    "> üîç \"If gradient descent isn‚Äôt working, set Œ± to a very **small** value (e.g., `0.0001`) and check if cost decreases at every step. If it doesn't, you may have a bug (like a wrong sign in the update rule).\"\n",
    "\n",
    "---\n",
    "\n",
    "### üö® Common Implementation Bug\n",
    "\n",
    "Make sure the update rule is:\n",
    "$$\n",
    "w := w - \\alpha \\cdot \\frac{\\partial J}{\\partial w}\n",
    "$$\n",
    "Not:\n",
    "$$\n",
    "w := w + \\alpha \\cdot \\frac{\\partial J}{\\partial w} \\quad \\text{‚ùå Wrong!}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### üìä How to Choose Œ± in Practice\n",
    "\n",
    "Try a **range of learning rates**, e.g.:\n",
    "\n",
    "- `Œ± = 0.001`\n",
    "- `Œ± = 0.003` (3x larger)\n",
    "- `Œ± = 0.01`\n",
    "- `Œ± = 0.03`\n",
    "- `Œ± = 0.1`\n",
    "- ...\n",
    "\n",
    "For each:\n",
    "- Run gradient descent for a few iterations\n",
    "- Plot the **cost vs. iteration**\n",
    "- Choose the largest Œ± that **still gives consistent decrease**\n",
    "\n",
    "---\n",
    "\n",
    "### üîÅ Summary of Good Strategy\n",
    "\n",
    "1. Start small (e.g., `0.001`)\n",
    "2. Multiply by ~3 each time until cost no longer decreases\n",
    "3. Pick the **largest Œ±** that still causes consistent descent\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Recommendation\n",
    "\n",
    "Also try the **optional lab** to:\n",
    "- Experiment with different Œ± values\n",
    "- See effects of **feature scaling** on convergence\n",
    "- Observe real plots of learning curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0905c3",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Feature Engineering\n",
    "\n",
    "The choice and design of features can significantly impact the performance of a learning algorithm. In many real-world scenarios, **feature engineering** ‚Äî the process of transforming raw input data into more meaningful representations ‚Äî is a **critical step** to improve predictive performance.\n",
    "\n",
    "---\n",
    "\n",
    "### üìç Example: Predicting House Prices\n",
    "\n",
    "Suppose we have two original features:\n",
    "\n",
    "- $x_1$: **Frontage** (width of the lot)\n",
    "- $x_2$: **Depth** (length of the lot)\n",
    "\n",
    "You could use these directly in a linear model:\n",
    "\n",
    "$$\n",
    "f_{\\mathbf{w},b}(x) = w_1 x_1 + w_2 x_2 + b\n",
    "$$\n",
    "\n",
    "However, you might notice that **area** (frontage √ó depth) is more predictive than width or depth separately:\n",
    "\n",
    "- Define a **new feature**: $x_3 = x_1 \\cdot x_2$ (the lot area)\n",
    "\n",
    "Now, your model becomes:\n",
    "\n",
    "$$\n",
    "f_{\\mathbf{w},b}(x) = w_1 x_1 + w_2 x_2 + w_3 x_3 + b\n",
    "$$\n",
    "\n",
    "This allows the algorithm to **learn the relative importance** of frontage, depth, and area.\n",
    "\n",
    "---\n",
    "\n",
    "### üîÅ What is Feature Engineering?\n",
    "\n",
    "Feature engineering is the process of:\n",
    "- Creating **new features** from existing ones.\n",
    "- Using **domain knowledge** or **intuition** to design better inputs.\n",
    "- Making the data **easier** for the model to understand and learn from.\n",
    "\n",
    "This can include:\n",
    "- Mathematical transformations (e.g., area, ratios)\n",
    "- Polynomial features (e.g., $x^2$, $x^3$)\n",
    "- Encoding categorical variables\n",
    "- Normalization or scaling\n",
    "\n",
    "---\n",
    "\n",
    "### üí° Why It Matters\n",
    "\n",
    "- Helps the model **fit better** (especially in small datasets)\n",
    "- Can capture **non-linear relationships**\n",
    "- Enables better **generalization** and predictive power\n",
    "\n",
    "In the next section, we‚Äôll see how **polynomial features** allow models to fit **non-linear functions** using linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4f91e7",
   "metadata": {},
   "source": [
    "## üìà Polynomial Regression & Feature Engineering\n",
    "\n",
    "### üß† Motivation\n",
    "\n",
    "So far, linear regression fits **straight lines** to data. But what if your data exhibits **non-linear patterns**?\n",
    "\n",
    "Using **polynomial regression**, you can extend linear models to capture **curves**, by introducing **higher-order features** (e.g., $x^2$, $x^3$, etc.).\n",
    "\n",
    "---\n",
    "\n",
    "### üè† Housing Price Example\n",
    "\n",
    "Suppose you want to predict house prices from **size (x)**. A linear model may not fit the data well:\n",
    "\n",
    "- **Linear model:**  \n",
    "  $$\n",
    "  f(x) = w_1x + b\n",
    "  $$  \n",
    "  ‚Üí Poor fit for curved trends.\n",
    "\n",
    "- **Quadratic model:**  \n",
    "  $$\n",
    "  f(x) = w_1x + w_2x^2 + b\n",
    "  $$  \n",
    "  ‚Üí May fit better, but goes back down for large $x$ (not realistic for house prices).\n",
    "\n",
    "- **Cubic model:**  \n",
    "  $$\n",
    "  f(x) = w_1x + w_2x^2 + w_3x^3 + b\n",
    "  $$  \n",
    "  ‚Üí Can model increasing trends more realistically.\n",
    "\n",
    "- **Square root model:**  \n",
    "  $$\n",
    "  f(x) = w_1x + w_2\\sqrt{x} + b\n",
    "  $$  \n",
    "  ‚Üí Models fast growth early, then flattens (another realistic option).\n",
    "\n",
    "---\n",
    "\n",
    "### üõ†Ô∏è What is Polynomial Regression?\n",
    "\n",
    "Polynomial regression **extends linear regression** by adding powers of the input features as new features.  \n",
    "Although the hypothesis is **non-linear in $x$**, it's **linear in the parameters** $w$.\n",
    "\n",
    "> ‚úÖ Still considered a linear model (in $w$), but allows modeling **non-linear relationships**.\n",
    "\n",
    "Example for one feature:\n",
    "- Input: $x$\n",
    "- Features: $[x, x^2, x^3]$\n",
    "- Model:  \n",
    "  $$f_{\\mathbf{w},b}(x) = w_1x + w_2x^2 + w_3x^3 + b$$\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öñÔ∏è Importance of Feature Scaling\n",
    "\n",
    "As you raise features to higher powers:\n",
    "- Their **range increases dramatically**\n",
    "  - e.g. if $x \\in [1, 1000]$  \n",
    "    ‚Üí $x^2 \\in [1, 10^6]$, $x^3 \\in [1, 10^9]$\n",
    "- This leads to **unstable gradients** and slow convergence\n",
    "\n",
    "‚úÖ Apply **feature scaling** (like Z-score normalization) before training with polynomial features.\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Summary: Key Ideas\n",
    "\n",
    "- Polynomial regression = **linear regression + feature engineering**\n",
    "- Helps fit **curved data**\n",
    "- You can choose from many transformations:\n",
    "  - $x^2$, $x^3$, $\\sqrt{x}$, $\\log(x)$, $x^{-1}$, etc.\n",
    "- Your choice of features has a **huge impact** on model performance\n",
    "- Try different feature sets and **evaluate model performance**\n",
    "- Scikit-learn makes it easy to implement polynomial regression (via `PolynomialFeatures`)\n",
    "\n",
    "---\n",
    "\n",
    "### üîç Practice Lab\n",
    "\n",
    "- Try coding your own polynomial regression\n",
    "- Then test using `scikit-learn` to validate your implementation\n",
    "- Explore how changing degree and learning rate ($\\alpha$) affects performance\n",
    "\n",
    "üéâ Congrats on finishing Week 2!  \n",
    "Next up: **Classification**, where we move from predicting numbers to **predicting categories**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a959cf77",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
