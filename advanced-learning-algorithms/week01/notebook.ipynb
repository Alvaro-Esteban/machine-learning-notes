{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9879fabb",
   "metadata": {},
   "source": [
    "# üß† Neural Networks ‚Äì Key Idea\n",
    "\n",
    "## üìå Limitations of Traditional Algorithms\n",
    "\n",
    "- Algorithms like **linear regression** or **logistic regression** have **limited performance**, even when given **more data**.\n",
    "- They don't **scale well** or take full advantage of **big data**.\n",
    "- This makes them ineffective for complex tasks like vision or natural language.\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Neural Networks vs. Traditional Algorithms\n",
    "\n",
    "- When training neural networks of increasing size:\n",
    "  - üü¢ **Small network** ‚Üí slight improvement.\n",
    "  - üü° **Medium-sized network** ‚Üí better performance.\n",
    "  - üî¥ **Large network** ‚Üí performance **keeps improving** with more data.\n",
    "- This ability to scale is **key** to their success.\n",
    "\n",
    "---\n",
    "\n",
    "## üìà Why Deep Learning Took Off\n",
    "\n",
    "- ‚úÖ Massive availability of **big data**.\n",
    "- ‚úÖ Ability to train **deep neural networks** (more layers, more neurons).\n",
    "- ‚úÖ Improved computational resources: **GPUs**, parallel processing.\n",
    "- Result: major breakthroughs in:\n",
    "  - üéôÔ∏è Speech recognition\n",
    "  - üì∏ Computer vision\n",
    "  - üí¨ Natural Language Processing (NLP)\n",
    "  - ‚öïÔ∏è And more biomedical applications...\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Conclusion\n",
    "\n",
    "> Deep neural networks can **leverage more data and compute** to improve performance ‚Äî something traditional algorithms can't do. This is why **deep learning** has become the dominant approach in many AI tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdf8156",
   "metadata": {},
   "source": [
    "# üß† Deeper Neural Networks ‚Äì Architecture & Intuition\n",
    "\n",
    "## üîÑ From Input to Output\n",
    "\n",
    "- A neural network takes an **input feature vector** $X$, processes it through **one or more hidden layers**, and finally produces a **single output** (e.g., a prediction).\n",
    "- Each **hidden layer** contains multiple **neurons** (aka *units*), which apply a weighted sum and activation function to the inputs.\n",
    "- The output of each layer becomes the **input to the next layer**.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Learning Features Automatically\n",
    "\n",
    "- Even if you *think* a network should compute certain features like ‚Äúaffordability,‚Äù ‚Äúawareness,‚Äù or ‚Äúquality,‚Äù **you don‚Äôt need to hand-design them**.\n",
    "- Neural networks **learn which features to extract** by themselves during training ‚Äî that‚Äôs one of their biggest advantages!\n",
    "- This automatic **feature extraction** is what makes neural networks powerful for complex tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## üèóÔ∏è Examples of Deeper Networks\n",
    "\n",
    "### üß± Example 1 ‚Äì 2 Hidden Layers\n",
    "- **Input layer** ‚Üí 3 neurons in **1st hidden layer** ‚Üí 2 neurons in **2nd hidden layer** ‚Üí **Output layer**\n",
    "- Activations flow forward through the network:  \n",
    "  $$\n",
    "  X \\rightarrow a^{[1]} \\rightarrow a^{[2]} \\rightarrow \\hat{y}$$\n",
    "\n",
    "### üß± Example 2 ‚Äì 3 Hidden Layers\n",
    "- Input layer  \n",
    "‚Üí Hidden Layer 1  \n",
    "‚Üí Hidden Layer 2  \n",
    "‚Üí Hidden Layer 3  \n",
    "‚Üí Output\n",
    "\n",
    "This is called a **deep neural network**, as it has **multiple layers**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Choosing Architecture: Layers & Units\n",
    "\n",
    "- When designing your own neural network, you must decide:\n",
    "  - How many **hidden layers** to include\n",
    "  - How many **neurons per layer**\n",
    "- These choices define the **architecture** of your neural network.\n",
    "- Choosing the right architecture can significantly affect the performance of your model.\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Terminology\n",
    "\n",
    "> **Multilayer Perceptron (MLP)** = a neural network with **one or more hidden layers**.\n",
    "\n",
    "If you see this term in books or papers, it refers to the kind of layered structure we've just described.\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Summary\n",
    "\n",
    "> Neural networks consist of layers of neurons that transform an input into an output. Deep networks (with multiple layers) can automatically learn useful features from data, and the choice of architecture (number of layers & neurons) plays a key role in their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b7d675",
   "metadata": {},
   "source": [
    "# üß† Forward Propagation ‚Äì Notation & Layer Computation\n",
    "\n",
    "## üß© Layer Numbering Convention\n",
    "\n",
    "- Input layer ‚Üí **Layer 0**\n",
    "- Hidden layers ‚Üí **Layers 1, 2, ..., L-1**\n",
    "- Output layer ‚Üí **Layer L**\n",
    "- So if a network has 3 hidden layers + 1 output layer ‚Üí total of **4 layers**\n",
    "- We **do not count the input layer** when saying ‚Äú4-layer neural network‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "## üî¢ Neuron-Level Notation\n",
    "\n",
    "For any layer $l$ and neuron (unit) $j$ in that layer:\n",
    "\n",
    "- $a_j^{[l]}$: Activation of neuron $j$ in layer $l$\n",
    "- $w_j^{[l]}$: Weight vector of neuron $j$ in layer $l$\n",
    "- $b_j^{[l]}$: Bias of neuron $j$ in layer $l$\n",
    "- $a^{[l-1]}$: Activation vector from the **previous layer**, used as input\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Activation Computation (per neuron)\n",
    "\n",
    "The activation of neuron $j$ in layer $l$ is computed as:\n",
    "\n",
    "$$\n",
    "a_j^{[l]} = g\\left( \\mathbf{w}_j^{[l]} \\cdot \\mathbf{a}^{[l-1]} + b_j^{[l]} \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $g$ is the **activation function**, e.g. sigmoid\n",
    "- $\\cdot$ is the **dot product**\n",
    "- $\\mathbf{a}^{[l-1]}$ is a vector ‚Üí the output of layer $l-1$\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Example: Layer 3 with 3 neurons\n",
    "\n",
    "Suppose:\n",
    "- Layer 3 has **3 neurons** ‚Üí $j = 1, 2, 3$\n",
    "- Input is vector $\\mathbf{a}^{[2]}$ (from Layer 2)\n",
    "\n",
    "Each neuron in Layer 3 computes:\n",
    "\n",
    "```text\n",
    "a_1^{[3]} = g(w_1^{[3]} ‚ãÖ a^{[2]} + b_1^{[3]})\n",
    "a_2^{[3]} = g(w_2^{[3]} ‚ãÖ a^{[2]} + b_2^{[3]})\n",
    "a_3^{[3]} = g(w_3^{[3]} ‚ãÖ a^{[2]} + b_3^{[3]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbd7ee4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
