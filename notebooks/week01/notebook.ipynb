{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2aa0a68",
   "metadata": {},
   "source": [
    "# üß™ Week 01 - Introduction to Machine Learning\n",
    "\n",
    "## üß† What is Machine Learning?\n",
    "\n",
    "Machine learning (ML) is a field of AI where computers learn from data instead of being explicitly programmed.  \n",
    "There are **two main types**:\n",
    "\n",
    "- **Supervised Learning** üéØ\n",
    "- **Unsupervised Learning** üîç\n",
    "\n",
    "Of these two, supervised learning is the most widely used in real-world applications and has seen the most rapid innovation.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Supervised Learning\n",
    "\n",
    "Supervised learning algorithms learn to map **inputs (x)** to **outputs (y)**, using example pairs (x, y).  \n",
    "The model is trained on data where the correct outputs are known, so it can later predict the output for new, unseen inputs.\n",
    "\n",
    "### üìå Example Tasks:\n",
    "- **Spam Detection**:  \n",
    "  Input: email text ‚Üí Output: spam (1) or not spam (0)\n",
    "  \n",
    "- **Speech Recognition**:  \n",
    "  Input: audio clip ‚Üí Output: transcript (text)\n",
    "\n",
    "---\n",
    "\n",
    "## üìà Types of Supervised Learning\n",
    "\n",
    "### üî¢ Regression\n",
    "- Predicts **continuous numeric values**.\n",
    "- Output: any number (infinite possibilities).\n",
    "- Example: Predicting house prices, temperature, age, etc.\n",
    "\n",
    "### üßÆ Classification\n",
    "- Predicts **discrete categories**.\n",
    "- Output: a class label (e.g., 0 or 1).\n",
    "- Example: Predicting if a tumor is **benign (0)** or **malignant (1)**.\n",
    "\n",
    "Key difference:\n",
    "> Regression ‚Üí infinite possible outputs  \n",
    "> Classification ‚Üí limited number of classes\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary\n",
    "\n",
    "| Task               | Input             | Output         | Type            |\n",
    "|--------------------|-------------------|----------------|-----------------|\n",
    "| Spam detection     | Email text        | 0 or 1         | Classification  |\n",
    "| House price        | Features of house | Price ($)      | Regression      |\n",
    "| Tumor diagnosis    | Medical data      | Benign / Malig | Classification  |\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ab6617",
   "metadata": {},
   "source": [
    "## üîç Unsupervised Learning\n",
    "\n",
    "In **unsupervised learning**, the algorithm is given **data without associated labels (y)**.\n",
    "\n",
    "For example, imagine a dataset with patient information:\n",
    "- Tumor size\n",
    "- Patient age  \n",
    "...but without knowing whether the tumor is **benign** or **malignant**.\n",
    "\n",
    "Here, we're not giving the algorithm the \"correct answer\" ‚Äî there's **no supervision**. Instead, we ask the algorithm to **find patterns or structure in the data** on its own.\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Clustering\n",
    "\n",
    "A common type of unsupervised learning is **clustering**, where the algorithm groups similar data points together based on some similarity.\n",
    "\n",
    "Example:\n",
    "- The algorithm might discover **two natural clusters** of patients, based on age and tumor size.\n",
    "- These clusters might represent different biological subtypes, even if the algorithm was never told that.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è Anomaly Detection\n",
    "\n",
    "Another important unsupervised learning technique is **anomaly detection**.\n",
    "\n",
    "It identifies **rare or unusual patterns** in data that don't fit the norm.  \n",
    "\n",
    "Used in:\n",
    "- üîê **Fraud detection** (e.g., abnormal credit card transactions)\n",
    "- üö® **Network security** (detecting intrusions)\n",
    "- üß™ **Medical diagnostics** (spotting outliers in patient data)\n",
    "\n",
    "---\n",
    "\n",
    "### üîΩ Dimensionality Reduction\n",
    "\n",
    "Dimensionality reduction compresses a dataset with **many features** into fewer dimensions while retaining as much **relevant information** as possible.\n",
    "\n",
    "Useful for:\n",
    "- Visualizing high-dimensional data in 2D or 3D\n",
    "- Reducing noise and overfitting\n",
    "- Speeding up learning algorithms\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary\n",
    "\n",
    "| Technique              | Goal                            | Example Use Case           |\n",
    "|------------------------|----------------------------------|----------------------------|\n",
    "| Clustering             | Group data into clusters         | Market segmentation        |\n",
    "| Anomaly Detection      | Detect unusual data points       | Fraud detection            |\n",
    "| Dimensionality Reduction | Compress data efficiently       | Visualizing gene expression |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b2ffec",
   "metadata": {},
   "source": [
    "# üìâ Linear Regression\n",
    "\n",
    "## üìå Problem Setup\n",
    "\n",
    "In **Supervised Learning**, the goal is to learn a function that maps **inputs (x)** to **outputs (y)** based on examples in a training set.\n",
    "\n",
    "For example, in **house price prediction**:\n",
    "\n",
    "- **Input (x)**: Size of the house (e.g. 2,104 sqft)\n",
    "- **Output (y)**: Price of the house (e.g. \\$400,000)\n",
    "\n",
    "Each training example is a pair:  \n",
    "**(x, y)** = (2104, 400)\n",
    "\n",
    "---\n",
    "\n",
    "### üî¢ Notation Summary\n",
    "\n",
    "| Symbol               | Meaning                              |\n",
    "|----------------------|--------------------------------------|\n",
    "| `x`                  | Input feature (e.g. house size)      |\n",
    "| `y`                  | Output target (e.g. house price)     |\n",
    "| `m`                  | Number of training examples          |\n",
    "| `(x‚ÅΩ‚Å±‚Åæ, y‚ÅΩ‚Å±‚Åæ)`       | The i-th training example             |\n",
    "| `≈∑` (y-hat)          | Prediction made by the model         |\n",
    "| `f(x)` or `f(x; w,b)`| Model function                       |\n",
    "| `w`                  | Weight (slope)                       |\n",
    "| `b`                  | Bias (intercept)                    |\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Linear Model\n",
    "\n",
    "The simplest form of a regression model is a **linear function**:\n",
    "\n",
    "\\[\n",
    "f(x) = {≈∑} = wx + b\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- `w` is the **weight** (how much x influences y)\n",
    "- `b` is the **bias** (the value of y when x = 0)\n",
    "\n",
    "This is called **Univariate Linear Regression** because it uses only one input feature.\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ How Learning Works\n",
    "\n",
    "You give the learning algorithm many examples of (x, y) and it learns the **best values of w and b** to minimize prediction error.\n",
    "\n",
    "> The output of the learning process is a function `f` that can predict `y` from new `x` values.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Key Takeaways\n",
    "\n",
    "- We use `(x‚ÅΩ‚Å±‚Åæ, y‚ÅΩ‚Å±‚Åæ)` to denote the i-th training example.\n",
    "- `m` = total number of training examples.\n",
    "- `≈∑` = predicted output from the model.\n",
    "- Goal: Learn parameters `(w, b)` so that predictions `≈∑ = wx + b` are as close as possible to actual outputs `y`.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e5f2ce",
   "metadata": {},
   "source": [
    "# üéØ Cost Function in Linear Regression\n",
    "\n",
    "To implement linear regression, the first step is to define a **cost function**, which quantifies **how well the model's predictions match the actual data**.\n",
    "\n",
    "---\n",
    "\n",
    "## üßÆ Model and Prediction\n",
    "\n",
    "Given a training example \\( (x^{(i)}, y^{(i)}) \\), the model's prediction is:\n",
    "\n",
    "$$\n",
    "\\hat{y}^{(i)} = f(x^{(i)}) = wx^{(i)} + b\n",
    "$$\n",
    "\n",
    "The **error** for that example is:\n",
    "\n",
    "$$\n",
    "\\text{Error}^{(i)} = \\hat{y}^{(i)} - y^{(i)}\n",
    "$$\n",
    "\n",
    "To penalize large errors (positive or negative), we square the difference:\n",
    "\n",
    "$$\n",
    "\\left( \\hat{y}^{(i)} - y^{(i)} \\right)^2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## üìê Cost Function Formula\n",
    "\n",
    "To evaluate the performance across all \\( m \\) training examples, we take the **average squared error**:\n",
    "\n",
    "$$\n",
    "J(w, b) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( f(x^{(i)}) - y^{(i)} \\right)^2\n",
    "$$\n",
    "\n",
    "Or, more formally using function notation:\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "\\[\n",
    "\\boxed{\n",
    "J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} \\left( f_{w,b}(x^{(i)}) - y^{(i)} \\right)^2\n",
    "}\n",
    "\\tag{1}\n",
    "\\]\n",
    "\n",
    "</div>\n",
    "\n",
    "Where:\n",
    "\n",
    "- \\( f_{w,b}(x^{(i)}) = wx^{(i)} + b \\) ‚Äî the model prediction  \n",
    "- \\( y^{(i)} \\) ‚Äî the actual value  \n",
    "- \\( m \\) ‚Äî number of training examples  \n",
    "- \\( J(w,b) \\) ‚Äî the cost function\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Why divide by \\( 2m \\)?\n",
    "\n",
    "The division by **2** is a mathematical convenience. It simplifies the derivative of the squared term during optimization (gradient descent):\n",
    "\n",
    "$$\n",
    "\\frac{d}{dw} \\left( \\frac{1}{2} (\\hat{y} - y)^2 \\right) = (\\hat{y} - y) \\cdot \\frac{d\\hat{y}}{dw}\n",
    "$$\n",
    "\n",
    "This makes formulas cleaner, but **does not affect the behavior** of the cost function.\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Goal of Training\n",
    "\n",
    "> Find the optimal values of \\( w \\) and \\( b \\) that **minimize the cost** \\( J(w, b) \\):\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "$$\n",
    "\\min_{w, b} \\ J(w, b)\n",
    "$$\n",
    "\n",
    "</div>\n",
    "\n",
    "This is the objective of **gradient descent**, the optimization algorithm you'll see next.\n",
    "\n",
    "---\n",
    "\n",
    "## üìà Visual Intuition\n",
    "\n",
    "- As you adjust \\( w \\) and \\( b \\), the prediction line \\( f(x) = wx + b \\) changes.\n",
    "- The **cost function** measures the average squared distance from each data point to the prediction line.\n",
    "- Lower cost ‚Üí better fit.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27c64fc",
   "metadata": {},
   "source": [
    "# üßÆ Gradient Descent\n",
    "\n",
    "Gradient Descent is a general optimization algorithm used to **minimize cost functions** in models with one or many parameters.\n",
    "\n",
    "For example, if your cost function is:\n",
    "\n",
    "$$\n",
    "J(w_1, w_2, ..., w_n, b)\n",
    "$$\n",
    "\n",
    "The goal is to find the values of \\( w_1, ..., w_n \\) and \\( b \\) that minimize \\( J \\).\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Basic Idea\n",
    "\n",
    "1. Start with initial guesses for the parameters (e.g., \\( w = 0 \\), \\( b = 0 \\))\n",
    "2. Iteratively update them to reduce the cost function \\( J(w, b) \\)\n",
    "3. Continue until convergence (i.e., until \\( J \\) reaches a minimum)\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ Gradient Descent Update Rule\n",
    "\n",
    "On each iteration:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "w &:= w - \\alpha \\cdot \\frac{\\partial J(w, b)}{\\partial w} \\\\\n",
    "b &:= b - \\alpha \\cdot \\frac{\\partial J(w, b)}{\\partial b}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- Learning rate:\n",
    "  $$\n",
    "  \\alpha\n",
    "  $$\n",
    "\n",
    "- Gradients:\n",
    "  $$\n",
    "  \\frac{\\partial J}{\\partial w}, \\quad \\frac{\\partial J}{\\partial b}\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "## üìâ Learning Rate alpha \n",
    "\n",
    "- A small alpha: slow learning (small steps)\n",
    "- A large alpha: fast learning, but risk of overshooting or divergence\n",
    "\n",
    "Example values: alpha = 0.1, 0.01, 0.001 \n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ Simultaneous Update\n",
    "\n",
    "Both parameters must be updated **simultaneously**, using the **original values** of \\( w \\) and \\( b \\) from the current iteration:\n",
    "\n",
    "```python\n",
    "# Pseudocode\n",
    "temp_w = w - alpha * dJ_dw\n",
    "temp_b = b - alpha * dJ_db\n",
    "\n",
    "w = temp_w\n",
    "b = temp_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c3238f",
   "metadata": {},
   "source": [
    "## üß≠ What Happens at the Minimum?\n",
    "\n",
    "During gradient descent, parameters are updated using the formula:\n",
    "\n",
    "$$\n",
    "w := w - \\alpha \\cdot \\frac{\\partial J(w, b)}{\\partial w}\n",
    "$$\n",
    "\n",
    "If the derivative = 0, then the update becomes:\n",
    "\n",
    "$$\n",
    "w := w - \\alpha \\cdot 0 = w\n",
    "$$\n",
    "\n",
    "This means:\n",
    "- The parameter \\( w \\) **remains unchanged**\n",
    "- The algorithm has reached a **local minimum** (or possibly the global minimum)\n",
    "\n",
    "---\n",
    "\n",
    "## ü™ú Natural Slowdown Near the Minimum\n",
    "\n",
    "As gradient descent progresses:\n",
    "- The derivative gets smaller\n",
    "- So the **step size** becomes smaller automatically\n",
    "- Eventually, the algorithm **converges** to the minimum\n",
    "\n",
    "Even with a **fixed learning rate alpha**, the steps shrink near the minimum due to the nature of the gradient.\n",
    "\n",
    "---\n",
    "\n",
    "## üìâ What Kind of Minimum?\n",
    "\n",
    "In general, cost functions might have:\n",
    "\n",
    "- **Multiple local minima**\n",
    "- **Saddle points**\n",
    "- **Global minima**\n",
    "\n",
    "However, in **linear regression with squared error cost**, the cost function:\n",
    "\n",
    "$$\n",
    "J(w, b) = \\frac{1}{2m} \\sum_{i=1}^{m} (wx^{(i)} + b - y^{(i)})^2\n",
    "$$\n",
    "\n",
    "is a **convex function**. This means:\n",
    "> ‚úÖ It has only **one global minimum**  \n",
    "> ‚ùå No local minima or saddle points\n",
    "\n",
    "---\n",
    "\n",
    "## üìê Gradient Descent with Linear Regression\n",
    "\n",
    "We use the **gradient of the cost function** to update the parameters. These gradients are derived using calculus:\n",
    "\n",
    "### üîπ Partial Derivatives\n",
    "\n",
    "- Gradient w.r.t. \\( w \\):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(w, b)}{\\partial w} = \\frac{1}{m} \\sum_{i=1}^{m} \\left( f_{w,b}(x^{(i)}) - y^{(i)} \\right) \\cdot x^{(i)}\n",
    "$$\n",
    "\n",
    "- Gradient w.r.t. \\( b \\):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(w, b)}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^{m} \\left( f_{w,b}(x^{(i)}) - y^{(i)} \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "$$\n",
    "f_{w,b}(x^{(i)}) = wx^{(i)} + b\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### üîÅ Final Gradient Descent Algorithm\n",
    "\n",
    "At each step:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "w &:= w - \\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^{m} \\left( wx^{(i)} + b - y^{(i)} \\right) \\cdot x^{(i)} \\\\\n",
    "b &:= b - \\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^{m} \\left( wx^{(i)} + b - y^{(i)} \\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This algorithm will:\n",
    "- Always converge to the global minimum (for linear regression)\n",
    "- Slow down naturally near the minimum\n",
    "- Stop changing parameters once the gradient becomes (near) zero\n",
    "\n",
    "---\n",
    "\n",
    "### üßÆ Batch Gradient Descent\n",
    "\n",
    "**Batch Gradient Descent** refers to the standard form of gradient descent where:\n",
    "\n",
    "> Every update of the model parameters uses **all training examples** at once.\n",
    "\n",
    "In other words, on each iteration, we compute the gradients using the **entire dataset**:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(w, b)}{\\partial w} = \\frac{1}{m} \\sum_{i=1}^{m} (f_{w,b}(x^{(i)}) - y^{(i)}) \\cdot x^{(i)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(w, b)}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^{m} (f_{w,b}(x^{(i)}) - y^{(i)})\n",
    "$$\n",
    "\n",
    "This is in contrast to other methods like:\n",
    "\n",
    "- **Stochastic Gradient Descent (SGD):** updates using **1 example at a time**\n",
    "- **Mini-Batch Gradient Descent:** uses **a small subset (batch)** of examples per update (e.g., 32 or 64)\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Characteristics of Batch Gradient Descent:\n",
    "\n",
    "| Feature                | Description                                  |\n",
    "|------------------------|----------------------------------------------|\n",
    "| Uses full dataset      | Yes ‚Äî every iteration uses all \\( m \\) examples |\n",
    "| Stability              | High (smooth convergence)                   |\n",
    "| Speed per iteration    | Slower (more computation per step)          |\n",
    "| Memory usage           | High ‚Äî must load entire dataset              |\n",
    "\n",
    "---\n",
    "\n",
    "> üîç **Note:**  \n",
    "> The name ‚Äúbatch‚Äù comes from this usage of the **entire batch of training examples** at every update step ‚Äî even if it may not sound intuitive."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
