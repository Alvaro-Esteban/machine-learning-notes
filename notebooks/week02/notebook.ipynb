{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c0e1159",
   "metadata": {},
   "source": [
    "## üè† Week 02 ‚Äì Regression with Multiple Input Variables\n",
    "\n",
    "In the original version of linear regression, you had a single feature \\( x \\), the size of the house, and you were able to predict \\( y \\), the price of the house.  \n",
    "The model was:\n",
    "\n",
    "$$\n",
    "f_{w,b}(x) = wx + b \\tag{1}\n",
    "$$\n",
    "\n",
    "But now, what if you did not only have the size of the house as a feature to predict the price, but also the **number of bedrooms**, **number of floors**, and **age of the home in years**?\n",
    "\n",
    "That gives you much more information to predict the price.\n",
    "\n",
    "---\n",
    "\n",
    "### üßÆ Notation\n",
    "\n",
    "Let:\n",
    "\n",
    "- $\\vec{x} = [x_1, x_2, x_3, x_4]$: the four input features (e.g. size, bedrooms, floors, age)  \n",
    "- $x_j$: the $j$-th feature, with $j = 1, 2, ..., n$  \n",
    "- $\\vec{x}^{(i)} = [x_1^{(i)}, x_2^{(i)}, ..., x_n^{(i)}]$: feature vector of the $i$-th training example  \n",
    "- $n$: number of features (e.g., $n = 4$)\n",
    "\n",
    "So for a training example \\( i \\), we have:\n",
    "\n",
    "$$\n",
    "f_{w,b}(x^{(i)}) = w_1 x_1^{(i)} + w_2 x_2^{(i)} + w_3 x_3^{(i)} + w_4 x_4^{(i)} + b \\tag{2}\n",
    "$$\n",
    "\n",
    "Or more generally, for \\( n \\) features:\n",
    "\n",
    "$$\n",
    "f_{w,b}(x^{(i)}) = \\sum_{j=1}^{n} w_j x_j^{(i)} + b \\tag{3}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### üìê Vectors Notation\n",
    "\n",
    "Let:\n",
    "\n",
    "- $\\vec{w} = [w_1, w_2, ..., w_n]$: vector of weights  \n",
    "- $\\vec{x}^{(i)} = [x_1^{(i)}, x_2^{(i)}, ..., x_n^{(i)}]$: feature vector  \n",
    "- $b$: bias (a scalar)\n",
    "\n",
    "Then the model becomes:\n",
    "\n",
    "$$\n",
    "f_{w,b}(x^{(i)}) = \\vec{w} \\cdot \\vec{x}^{(i)} + b \\tag{4}\n",
    "$$\n",
    "\n",
    "This uses the **dot product** of two vectors:\n",
    "\n",
    "$$\n",
    "\\vec{w} \\cdot \\vec{x}^{(i)} = \\sum_{j=1}^{n} w_j x_j^{(i)} \\tag{5}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Name of the Model\n",
    "\n",
    "This model is called **multiple linear regression** (not *multivariate regression* ‚Äî that refers to predicting multiple outputs, which is a different topic).\n",
    "\n",
    "It's the natural extension of **univariate linear regression** (with one feature) to multiple input features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b74954",
   "metadata": {},
   "source": [
    "## üßÆ Vectorization\n",
    "\n",
    "Vectorization is a technique to implement learning algorithms **more efficiently** ‚Äî both in terms of **execution time** and **code clarity**. It allows you to take advantage of optimized **linear algebra libraries** (e.g., NumPy) and even **GPU acceleration**.\n",
    "\n",
    "---\n",
    "\n",
    "### Parameters and Features\n",
    "\n",
    "Let:\n",
    "\n",
    "- $\\vec{w} = [w_1, w_2, w_3]$ with $n = 3$: weight vector  \n",
    "- $\\vec{x} = [x_1, x_2, x_3]$: feature vector  \n",
    "- $b$: bias (a scalar)\n",
    "\n",
    "In NumPy (Python):\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "w = np.array([1.0, 2.5, -3.3])\n",
    "x = np.array([10, 20, 30])\n",
    "b = 4\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå Without Vectorization\n",
    "\n",
    "**Manual computation (bad for large $n$):**\n",
    "\n",
    "$$\n",
    "f_{\\vec{w}, b}(\\vec{x}) = w_1 x_1 + w_2 x_2 + w_3 x_3 + b\n",
    "$$\n",
    "\n",
    "**Using a for loop:**\n",
    "\n",
    "```python\n",
    "f = 0\n",
    "for j in range(0, n):\n",
    "    f += w[j] * x[j]\n",
    "f += b\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ With Vectorization\n",
    "\n",
    "**Mathematical expression:**\n",
    "\n",
    "$$\n",
    "f_{\\vec{w}, b}(\\vec{x}) = \\vec{w} \\cdot \\vec{x} + b\n",
    "$$\n",
    "\n",
    "**Python (NumPy):**\n",
    "\n",
    "```python\n",
    "f = np.dot(w, x) + b\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ Benefits of Vectorization\n",
    "\n",
    "- ‚úÖ **Shorter, cleaner code**\n",
    "- ‚úÖ **Faster execution**, especially for large $n$\n",
    "- ‚úÖ Utilizes optimized backend libraries (e.g., BLAS, LAPACK, GPUs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a8f175",
   "metadata": {},
   "source": [
    "## Gradient Descent for Multiple Linear Regression\n",
    "\n",
    "We're going to repeatedly update each parameter $w_j$ using the rule:\n",
    "\n",
    "$$\n",
    "w_j := w_j - \\alpha \\frac{\\partial J(\\vec{w}, b)}{\\partial w_j}\n",
    "$$\n",
    "\n",
    "And the bias $b$ using:\n",
    "\n",
    "$$\n",
    "b := b - \\alpha \\frac{\\partial J(\\vec{w}, b)}{\\partial b}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\alpha$ is the learning rate  \n",
    "- $J(\\vec{w}, b)$ is the cost function  \n",
    "- $w_j$ refers to the $j$-th weight in the parameter vector $\\vec{w}$  \n",
    "\n",
    "---\n",
    "\n",
    "### üß† Intuition\n",
    "\n",
    "In univariate regression (one feature), we had the update rules:\n",
    "\n",
    "$$\n",
    "w := w - \\alpha \\frac{\\partial J(w, b)}{\\partial w}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b := b - \\alpha \\frac{\\partial J(w, b)}{\\partial b}\n",
    "$$\n",
    "\n",
    "Now, with **multiple features** ($n \\geq 2$), we update **each weight** $w_j$ individually:\n",
    "\n",
    "$$\n",
    "w_j := w_j - \\alpha \\left( \\frac{1}{m} \\sum_{i=1}^{m} \\left( f_{\\vec{w}, b}(\\vec{x}^{(i)}) - y^{(i)} \\right) \\cdot x_j^{(i)} \\right)\n",
    "$$\n",
    "\n",
    "And we update $b$ as:\n",
    "\n",
    "$$\n",
    "b := b - \\alpha \\left( \\frac{1}{m} \\sum_{i=1}^{m} \\left( f_{\\vec{w}, b}(\\vec{x}^{(i)}) - y^{(i)} \\right) \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $f_{\\vec{w}, b}(\\vec{x}^{(i)}) = \\vec{w} \\cdot \\vec{x}^{(i)} + b$ is the prediction\n",
    "- $x_j^{(i)}$ is the $j$-th feature of the $i$-th example\n",
    "- $m$ is the number of training examples\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ Implementation note\n",
    "\n",
    "Instead of hardcoding 3 or 4 parameters, you‚Äôll now use **loops over $j = 1..n$** to update all weights $\\vec{w}$. The update for $b$ stays similar.\n",
    "\n",
    "---\n",
    "\n",
    "### üßÆ Bonus: The Normal Equation\n",
    "\n",
    "There's another method that can compute the **optimal weights $\\vec{w}$ and bias $b$ without iterations**, called the **normal equation**. It's based on linear algebra.\n",
    "\n",
    "‚úÖ Pros:\n",
    "- No need to choose learning rate $\\alpha$\n",
    "- No need for iteration\n",
    "\n",
    "‚ö†Ô∏è Cons:\n",
    "- Computationally expensive for large $n$\n",
    "- Not generalizable to other algorithms (like logistic regression or neural networks)\n",
    "\n",
    "That's why in practice, most implementations still use **gradient descent** or **advanced solvers**, even for linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b362d7",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è Feature Scaling\n",
    "\n",
    "Feature scaling is a technique used to **normalize the range of independent variables or features** in your data. It ensures that all features contribute equally to the learning algorithm, especially for **gradient descent**, where features with larger scales can dominate the cost function and slow down convergence.\n",
    "\n",
    "### üìå Why is feature scaling important?\n",
    "\n",
    "- Features may have **different units** (e.g., size in m¬≤, age in years, price in $).\n",
    "- Without scaling, **gradient descent may converge slowly** or get stuck.\n",
    "- It helps to **improve numerical stability**.\n",
    "- Needed for algorithms that compute distances (e.g., **k-NN**, **SVM**, **logistic regression**).\n",
    "\n",
    "### ‚úÖ When to scale\n",
    "\n",
    "Feature scaling is **recommended** when:\n",
    "- You're using **gradient-based algorithms** (like linear regression, logistic regression, neural networks).\n",
    "- You have features with **different magnitudes** or units.\n",
    "- Your model does not handle feature scaling internally.\n",
    "\n",
    "---\n",
    "\n",
    "### üî¢ Common Methods of Feature Scaling\n",
    "\n",
    "Feature scaling ensures that all input features contribute equally to the model by adjusting their ranges or distributions. Here are the most common methods:\n",
    "\n",
    "- **Min-Max Scaling:** Subtract the minimum and divide by the range (max - min). Rescales features to lie between 0 and 1.\n",
    "- **Mean Normalization:** Subtract the mean and divide by the range. Centers the data around 0.\n",
    "- **Z-score Normalization (Standardization):** Subtract the mean and divide by the standard deviation. Ensures zero mean and unit variance.\n",
    "\n",
    "| Method                 | Formula                                                  | Typical Range        |\n",
    "|------------------------|----------------------------------------------------------|----------------------|\n",
    "| **Min-Max Scaling**    | $x' = \\dfrac{x - \\min(x)}{\\max(x) - \\min(x)}$            | $[0,\\ 1]$            |\n",
    "| **Mean Normalization** | $x' = \\dfrac{x - \\mu}{\\max(x) - \\min(x)}$                | Around 0             |\n",
    "| **Standardization**    | $x' = \\dfrac{x - \\mu}{\\sigma}$                           | Mean = 0, Std = 1    |\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Example in Python (Standardization)\n",
    "\n",
    "```python\n",
    "# Assuming X is a NumPy array of shape (m, n)\n",
    "mu     = np.mean(X, axis=0)   # mean for each feature\n",
    "sigma  = np.std(X, axis=0)    # standard deviation\n",
    "X_norm = (X - mu) / sigma     # standardized features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7420c769",
   "metadata": {},
   "source": [
    "## üìâ How to Tell if Gradient Descent is Converging\n",
    "\n",
    "To verify if gradient descent is working properly (i.e., minimizing the cost function \\( J(w, b) \\)), it helps to analyze the **learning curve** ‚Äî a plot of the cost function over iterations.\n",
    "\n",
    "### üîç Key Concepts\n",
    "\n",
    "- **Gradient Descent Update Rule:**\n",
    "  $$ w := w - \\alpha \\frac{\\partial J(w, b)}{\\partial w}, \\quad b := b - \\alpha \\frac{\\partial J(w, b)}{\\partial b} $$\n",
    "\n",
    "- **Learning Curve:**\n",
    "  - Horizontal axis: Number of iterations\n",
    "  - Vertical axis: Cost function \\( J(w, b) \\)\n",
    "  - Each point: Value of the cost after one update of \\( w \\) and \\( b \\)\n",
    "  - Purpose: Visualize if the cost is decreasing steadily\n",
    "\n",
    "### ‚úÖ Signs Gradient Descent is Working\n",
    "\n",
    "- The cost \\( J(w, b) \\) **decreases after every iteration**\n",
    "- The curve **smoothly flattens out**, indicating convergence\n",
    "- Cost function becomes stable (i.e., little to no change)\n",
    "\n",
    "### ‚ùå Signs Something is Wrong\n",
    "\n",
    "- Cost **increases** after an iteration ‚Üí likely:\n",
    "  - Learning rate \\( \\alpha \\) is too large\n",
    "  - Bug in the implementation\n",
    "- Curve is **not flattening** ‚Üí may not be converging\n",
    "\n",
    "### üìà Convergence Criteria\n",
    "\n",
    "- **Visual inspection** is the most reliable:\n",
    "  - Helps detect problems early (e.g., bad learning rate)\n",
    "  - Shows whether training should stop\n",
    "- **Automatic test (epsilon rule):**\n",
    "  - Define a small threshold \\( \\varepsilon \\), e.g., 0.001\n",
    "  - If cost decrease \\( < \\varepsilon \\), declare convergence\n",
    "  - Less preferred due to difficulty in choosing a good \\( \\varepsilon \\)\n",
    "\n",
    "### ‚è≥ Number of Iterations\n",
    "\n",
    "- Varies **greatly** depending on the application:\n",
    "  - Some models converge in 30 iterations\n",
    "  - Others may take 1,000 or 100,000\n",
    "- Hence, plotting the learning curve is more reliable than predefining a number\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89edcac",
   "metadata": {},
   "source": [
    "## üîß Choosing a Good Learning Rate (Œ±)\n",
    "\n",
    "Selecting the right learning rate is **crucial** for gradient descent to work effectively.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† What Can Go Wrong?\n",
    "\n",
    "#### üî∏ Learning rate too **large**:\n",
    "- Gradient descent **does not converge**\n",
    "- Cost function `J(w,b)` may **oscillate** or even **increase**\n",
    "- Can **overshoot** the minimum repeatedly\n",
    "- Possible graph shape: zig-zag or upward curve\n",
    "\n",
    "#### üî∏ Learning rate too **small**:\n",
    "- Gradient descent **converges very slowly**\n",
    "- Training takes many iterations\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Good Gradient Descent Behavior\n",
    "\n",
    "- Cost \\( J(w,b) \\) **decreases on every iteration**\n",
    "- Learning curve is smooth and decreasing\n",
    "- Converges towards a stable minimum\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ Debugging Tip\n",
    "\n",
    "> üîç \"If gradient descent isn‚Äôt working, set Œ± to a very **small** value (e.g., `0.0001`) and check if cost decreases at every step. If it doesn't, you may have a bug (like a wrong sign in the update rule).\"\n",
    "\n",
    "---\n",
    "\n",
    "### üö® Common Implementation Bug\n",
    "\n",
    "Make sure the update rule is:\n",
    "$$\n",
    "w := w - \\alpha \\cdot \\frac{\\partial J}{\\partial w}\n",
    "$$\n",
    "Not:\n",
    "$$\n",
    "w := w + \\alpha \\cdot \\frac{\\partial J}{\\partial w} \\quad \\text{‚ùå Wrong!}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### üìä How to Choose Œ± in Practice\n",
    "\n",
    "Try a **range of learning rates**, e.g.:\n",
    "\n",
    "- `Œ± = 0.001`\n",
    "- `Œ± = 0.003` (3x larger)\n",
    "- `Œ± = 0.01`\n",
    "- `Œ± = 0.03`\n",
    "- `Œ± = 0.1`\n",
    "- ...\n",
    "\n",
    "For each:\n",
    "- Run gradient descent for a few iterations\n",
    "- Plot the **cost vs. iteration**\n",
    "- Choose the largest Œ± that **still gives consistent decrease**\n",
    "\n",
    "---\n",
    "\n",
    "### üîÅ Summary of Good Strategy\n",
    "\n",
    "1. Start small (e.g., `0.001`)\n",
    "2. Multiply by ~3 each time until cost no longer decreases\n",
    "3. Pick the **largest Œ±** that still causes consistent descent\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Recommendation\n",
    "\n",
    "Also try the **optional lab** to:\n",
    "- Experiment with different Œ± values\n",
    "- See effects of **feature scaling** on convergence\n",
    "- Observe real plots of learning curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0905c3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
